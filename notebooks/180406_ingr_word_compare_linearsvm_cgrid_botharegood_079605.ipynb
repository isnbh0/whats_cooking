{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "with open('../_data/train.json', 'r') as f:\n",
    "    train = json.load(f)\n",
    "with open('../_data/test.json', 'r') as f:\n",
    "    test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39774, 9944)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'greek',\n",
       " 'id': 10259,\n",
       " 'ingredients': ['romaine lettuce',\n",
       "  'black olives',\n",
       "  'grape tomatoes',\n",
       "  'garlic',\n",
       "  'pepper',\n",
       "  'purple onion',\n",
       "  'seasoning',\n",
       "  'garbanzo beans',\n",
       "  'feta cheese crumbles']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'!': 34,\n",
       "         '%': 394,\n",
       "         '&': 479,\n",
       "         \"'\": 240,\n",
       "         '(': 55,\n",
       "         ')': 55,\n",
       "         ',': 814,\n",
       "         '.': 57,\n",
       "         '/': 2,\n",
       "         '®': 244,\n",
       "         '’': 8,\n",
       "         '€': 1,\n",
       "         '™': 79})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPEC_CHARS = re.compile(r'[^\\w\\s\\-_]')\n",
    "\n",
    "chars = [re.findall(SPEC_CHARS, x)\\\n",
    " for ilist in [r['ingredients'] for r in train+test] for x in ilist if re.search(SPEC_CHARS, x)]\n",
    "\n",
    "Counter([x for charlist in chars for x in charlist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean rules\n",
    "* remove `'`, `’`, `( oz*)`, `(`, `)`\n",
    "* replace `&` with `and`\n",
    "* replace all else (`™`, `®`, `.`, `€`) with `' '`\n",
    "* keep `%`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SPEC_REMOVE = re.compile(r'(\\'|\\’|\\(.*oz.*\\)|(\\()|(\\)))')\n",
    "SPEC_AND = re.compile(r'\\&')\n",
    "SPEC_ELSE = re.compile(r'[^\\w\\s\\%_]')\n",
    "\n",
    "def clean_ingr(ingr):\n",
    "    ingr = re.sub(SPEC_REMOVE, '', ingr)\n",
    "    ingr = re.sub(SPEC_AND, 'and', ingr)\n",
    "    ingr = re.sub(SPEC_ELSE, ' ', ingr)\n",
    "    return ' '.join(ingr.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline using TFIDF (bag of ingredients) and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def itself(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SPEC_REMOVE = re.compile(r'(\\'|\\’|\\(.*oz.*\\)|(\\()|(\\)))')\n",
    "SPEC_AND = re.compile(r'\\&')\n",
    "SPEC_ELSE = re.compile(r'[^\\w\\s\\%_]')\n",
    "\n",
    "def clean_ingr(ingr):\n",
    "    ingr = re.sub(SPEC_REMOVE, '', ingr)\n",
    "    ingr = re.sub(SPEC_AND, 'and', ingr)\n",
    "    ingr = re.sub(SPEC_ELSE, ' ', ingr)\n",
    "    return ' '.join(ingr.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ingrs(given):\n",
    "    ingrs = [[clean_ingr(i).lower() for i in recipe['ingredients']] for recipe in given]\n",
    "    return ingrs\n",
    "\n",
    "def get_labels(given):\n",
    "    return [r['cuisine'] for r in given]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ingrs = get_ingrs(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_words(ilist):\n",
    "    return ' '.join(ilist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ingr_word = Pipeline([\n",
    "    ('union', FeatureUnion([\n",
    "        (\"ingrs\", TfidfVectorizer(strip_accents='unicode',\n",
    "                                  tokenizer=itself,\n",
    "                                  preprocessor=itself)),\n",
    "        (\"words\", TfidfVectorizer(strip_accents='unicode',\n",
    "                                  preprocessor=combine_words,\n",
    "                                  stop_words='english')),\n",
    "        ])),\n",
    "    (\"linear svc\", LinearSVC(loss='hinge', C=10**0.1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word = Pipeline([\n",
    "    (\"words\", TfidfVectorizer(strip_accents='unicode',\n",
    "                              preprocessor=combine_words,\n",
    "                              ngram_range=(1, 2))),\n",
    "    (\"linear svc\", LinearSVC(loss='hinge', C=10**0.1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ingrs = get_ingrs(train)\n",
    "train_labels = get_labels(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using stop words\n",
    "## Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV]  ................................................................\n",
      "[CV]  ................................................................\n",
      "[CV]  ................................................................\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=0.7893413775766717, total=  12.5s\n",
      "[CV] ....................... , score=0.7852298417483045, total=  13.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   13.6s remaining:   20.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... , score=0.7856873349264244, total=  12.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:   14.0s remaining:    9.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... , score=0.7977853277966528, total=  13.1s\n",
      "[CV] ....................... , score=0.7931641115858256, total=  14.0s\n",
      "CPU times: user 1.12 s, sys: 108 ms, total: 1.23 s\n",
      "Wall time: 15 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   14.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   14.7s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores = cross_val_score(ingr_word, train_ingrs, train_labels, cv=5, n_jobs=-1, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.78522984,  0.79316411,  0.78934138,  0.78568733,  0.79778533])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7902415987267758"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV]  ................................................................\n",
      "[CV]  ................................................................\n",
      "[CV]  ................................................................\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=0.7937924101533048, total=  11.8s\n",
      "[CV] ....................... , score=0.7901281085154483, total=  12.4s\n",
      "[CV] ....................... , score=0.7914936454007802, total=  11.7s\n",
      "[CV] ....................... , score=0.7806565211923029, total=  12.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   12.9s remaining:   19.4s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:   13.0s remaining:    8.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... , score=0.7859477124183006, total=  12.5s\n",
      "CPU times: user 1.09 s, sys: 156 ms, total: 1.24 s\n",
      "Wall time: 13.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   13.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   13.4s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores = cross_val_score(word, train_ingrs, train_labels, cv=5, n_jobs=-1, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.79012811,  0.79379241,  0.78594771,  0.78065652,  0.79149365])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78840367953602741"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ingr_word1 = Pipeline([\n",
    "    ('union', FeatureUnion([\n",
    "        (\"ingrs\", TfidfVectorizer(strip_accents='unicode',\n",
    "                                  tokenizer=itself,\n",
    "                                  preprocessor=itself)),\n",
    "        (\"words\", TfidfVectorizer(strip_accents='unicode',\n",
    "                                  preprocessor=combine_words)),\n",
    "        ])),\n",
    "    (\"linear svc\", LinearSVC(loss='hinge', C=10**0.1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear svc': LinearSVC(C=1.2589254117941673, class_weight=None, dual=True,\n",
       "      fit_intercept=True, intercept_scaling=1, loss='hinge', max_iter=1000,\n",
       "      multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "      verbose=0),\n",
       " 'linear svc__C': 1.2589254117941673,\n",
       " 'linear svc__class_weight': None,\n",
       " 'linear svc__dual': True,\n",
       " 'linear svc__fit_intercept': True,\n",
       " 'linear svc__intercept_scaling': 1,\n",
       " 'linear svc__loss': 'hinge',\n",
       " 'linear svc__max_iter': 1000,\n",
       " 'linear svc__multi_class': 'ovr',\n",
       " 'linear svc__penalty': 'l2',\n",
       " 'linear svc__random_state': None,\n",
       " 'linear svc__tol': 0.0001,\n",
       " 'linear svc__verbose': 0,\n",
       " 'memory': None,\n",
       " 'steps': [('union', FeatureUnion(n_jobs=1,\n",
       "          transformer_list=[('ingrs', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2',\n",
       "           preprocessor=<func...f=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, use_idf=True, vocabulary=None))],\n",
       "          transformer_weights=None)),\n",
       "  ('linear svc', LinearSVC(C=1.2589254117941673, class_weight=None, dual=True,\n",
       "        fit_intercept=True, intercept_scaling=1, loss='hinge', max_iter=1000,\n",
       "        multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "        verbose=0))],\n",
       " 'union': FeatureUnion(n_jobs=1,\n",
       "        transformer_list=[('ingrs', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2',\n",
       "         preprocessor=<func...f=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, use_idf=True, vocabulary=None))],\n",
       "        transformer_weights=None),\n",
       " 'union__ingrs': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2',\n",
       "         preprocessor=<function itself at 0x7f9993eacf28>, smooth_idf=True,\n",
       "         stop_words=None, strip_accents='unicode', sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function itself at 0x7f9993eacf28>, use_idf=True,\n",
       "         vocabulary=None),\n",
       " 'union__ingrs__analyzer': 'word',\n",
       " 'union__ingrs__binary': False,\n",
       " 'union__ingrs__decode_error': 'strict',\n",
       " 'union__ingrs__dtype': numpy.int64,\n",
       " 'union__ingrs__encoding': 'utf-8',\n",
       " 'union__ingrs__input': 'content',\n",
       " 'union__ingrs__lowercase': True,\n",
       " 'union__ingrs__max_df': 1.0,\n",
       " 'union__ingrs__max_features': None,\n",
       " 'union__ingrs__min_df': 1,\n",
       " 'union__ingrs__ngram_range': (1, 1),\n",
       " 'union__ingrs__norm': 'l2',\n",
       " 'union__ingrs__preprocessor': <function __main__.itself>,\n",
       " 'union__ingrs__smooth_idf': True,\n",
       " 'union__ingrs__stop_words': None,\n",
       " 'union__ingrs__strip_accents': 'unicode',\n",
       " 'union__ingrs__sublinear_tf': False,\n",
       " 'union__ingrs__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'union__ingrs__tokenizer': <function __main__.itself>,\n",
       " 'union__ingrs__use_idf': True,\n",
       " 'union__ingrs__vocabulary': None,\n",
       " 'union__n_jobs': 1,\n",
       " 'union__transformer_list': [('ingrs',\n",
       "   TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2',\n",
       "           preprocessor=<function itself at 0x7f9993eacf28>, smooth_idf=True,\n",
       "           stop_words=None, strip_accents='unicode', sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function itself at 0x7f9993eacf28>, use_idf=True,\n",
       "           vocabulary=None)),\n",
       "  ('words',\n",
       "   TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2',\n",
       "           preprocessor=<function combine_words at 0x7f9993eb7598>,\n",
       "           smooth_idf=True, stop_words=None, strip_accents='unicode',\n",
       "           sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, use_idf=True, vocabulary=None))],\n",
       " 'union__transformer_weights': None,\n",
       " 'union__words': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2',\n",
       "         preprocessor=<function combine_words at 0x7f9993eb7598>,\n",
       "         smooth_idf=True, stop_words=None, strip_accents='unicode',\n",
       "         sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, use_idf=True, vocabulary=None),\n",
       " 'union__words__analyzer': 'word',\n",
       " 'union__words__binary': False,\n",
       " 'union__words__decode_error': 'strict',\n",
       " 'union__words__dtype': numpy.int64,\n",
       " 'union__words__encoding': 'utf-8',\n",
       " 'union__words__input': 'content',\n",
       " 'union__words__lowercase': True,\n",
       " 'union__words__max_df': 1.0,\n",
       " 'union__words__max_features': None,\n",
       " 'union__words__min_df': 1,\n",
       " 'union__words__ngram_range': (1, 1),\n",
       " 'union__words__norm': 'l2',\n",
       " 'union__words__preprocessor': <function __main__.combine_words>,\n",
       " 'union__words__smooth_idf': True,\n",
       " 'union__words__stop_words': None,\n",
       " 'union__words__strip_accents': 'unicode',\n",
       " 'union__words__sublinear_tf': False,\n",
       " 'union__words__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'union__words__tokenizer': None,\n",
       " 'union__words__use_idf': True,\n",
       " 'union__words__vocabulary': None}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingr_word1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    "    'union__ingrs__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "    'union__ingrs__stop_words': [None, 'english']\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator=ingr_word1,\n",
    "                        param_grid=param_test1,\n",
    "                        scoring='accuracy',\n",
    "                        n_jobs=4,\n",
    "                        iid=False,\n",
    "                        cv=3,\n",
    "                        verbose=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import strftime\n",
    "\n",
    "def print_time():\n",
    "    print(strftime('%y%m%d-%H%M%S'))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180406-185625\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV] union__ingrs__ngram_range=(1, 1), union__ingrs__stop_words=None .\n",
      "[CV] union__ingrs__ngram_range=(1, 1), union__ingrs__stop_words=None .\n",
      "[CV] union__ingrs__ngram_range=(1, 1), union__ingrs__stop_words=None .\n",
      "[CV] union__ingrs__ngram_range=(1, 1), union__ingrs__stop_words=english \n",
      "[CV]  union__ingrs__ngram_range=(1, 1), union__ingrs__stop_words=None, score=0.7845991402066521, total=   8.4s\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:   10.4s\n",
      "[CV] union__ingrs__ngram_range=(1, 1), union__ingrs__stop_words=english \n",
      "[CV]  union__ingrs__ngram_range=(1, 1), union__ingrs__stop_words=None, score=0.7879816029555907, total=   9.3s\n",
      "[Parallel(n_jobs=4)]: Done   2 tasks      | elapsed:   11.2s\n",
      "[CV] union__ingrs__ngram_range=(1, 1), union__ingrs__stop_words=english \n",
      "[CV]  union__ingrs__ngram_range=(1, 1), union__ingrs__stop_words=None, score=0.7905221853305161, total=   9.2s\n",
      "[Parallel(n_jobs=4)]: Done   3 tasks      | elapsed:   11.7s\n",
      "[CV] union__ingrs__ngram_range=(1, 2), union__ingrs__stop_words=None .\n",
      "[CV]  union__ingrs__ngram_range=(1, 1), union__ingrs__stop_words=english, score=0.7879816029555907, total=   9.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 tasks      | elapsed:   12.0s\n",
      "[CV] union__ingrs__ngram_range=(1, 2), union__ingrs__stop_words=None .\n",
      "[CV]  union__ingrs__ngram_range=(1, 1), union__ingrs__stop_words=english, score=0.7846745606757674, total=   7.9s\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:   20.5s\n",
      "[CV] union__ingrs__ngram_range=(1, 2), union__ingrs__stop_words=None .\n",
      "[CV]  union__ingrs__ngram_range=(1, 1), union__ingrs__stop_words=english, score=0.7905221853305161, total=   8.4s\n",
      "[CV] union__ingrs__ngram_range=(1, 2), union__ingrs__stop_words=english \n",
      "[Parallel(n_jobs=4)]: Done   6 tasks      | elapsed:   21.7s\n",
      "[CV]  union__ingrs__ngram_range=(1, 2), union__ingrs__stop_words=None, score=0.7874651180330342, total=  12.4s\n",
      "[Parallel(n_jobs=4)]: Done   7 tasks      | elapsed:   27.0s\n",
      "[CV]  union__ingrs__ngram_range=(1, 2), union__ingrs__stop_words=None, score=0.7916006936590515, total=  12.9s\n",
      "[CV] union__ingrs__ngram_range=(1, 2), union__ingrs__stop_words=english \n",
      "[Parallel(n_jobs=4)]: Done   8 tasks      | elapsed:   27.3s\n",
      "[CV] union__ingrs__ngram_range=(1, 2), union__ingrs__stop_words=english \n",
      "[CV]  union__ingrs__ngram_range=(1, 2), union__ingrs__stop_words=None, score=0.7945970419559312, total=  11.1s\n",
      "[Parallel(n_jobs=4)]: Done   9 tasks      | elapsed:   34.3s\n",
      "[CV] union__ingrs__ngram_range=(1, 3), union__ingrs__stop_words=None .\n",
      "[CV]  union__ingrs__ngram_range=(1, 2), union__ingrs__stop_words=english, score=0.7916006936590515, total=  13.4s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   38.2s\n",
      "[CV] union__ingrs__ngram_range=(1, 3), union__ingrs__stop_words=None .\n",
      "[CV]  union__ingrs__ngram_range=(1, 2), union__ingrs__stop_words=english, score=0.7945970419559312, total=  10.7s\n",
      "[Parallel(n_jobs=4)]: Done  11 tasks      | elapsed:   40.6s\n",
      "[CV] union__ingrs__ngram_range=(1, 3), union__ingrs__stop_words=None .\n",
      "[CV]  union__ingrs__ngram_range=(1, 2), union__ingrs__stop_words=english, score=0.7874651180330342, total=  12.3s\n",
      "[Parallel(n_jobs=4)]: Done  12 tasks      | elapsed:   41.9s\n",
      "[CV] union__ingrs__ngram_range=(1, 3), union__ingrs__stop_words=english \n",
      "[CV]  union__ingrs__ngram_range=(1, 3), union__ingrs__stop_words=None, score=0.7943150116866471, total=  14.3s\n",
      "[Parallel(n_jobs=4)]: Done  13 tasks      | elapsed:   52.1s\n",
      "[CV] union__ingrs__ngram_range=(1, 3), union__ingrs__stop_words=english \n",
      "[CV]  union__ingrs__ngram_range=(1, 3), union__ingrs__stop_words=None, score=0.7907081982049928, total=  16.2s\n",
      "[Parallel(n_jobs=4)]: Done  14 tasks      | elapsed:   57.7s\n",
      "[CV] union__ingrs__ngram_range=(1, 3), union__ingrs__stop_words=english \n",
      "[CV]  union__ingrs__ngram_range=(1, 3), union__ingrs__stop_words=None, score=0.797011771808029, total=  15.1s\n",
      "[Parallel(n_jobs=4)]: Done  15 tasks      | elapsed:   59.0s\n",
      "[CV] union__ingrs__ngram_range=(1, 4), union__ingrs__stop_words=None .\n",
      "[CV]  union__ingrs__ngram_range=(1, 3), union__ingrs__stop_words=english, score=0.7943150116866471, total=  15.5s\n",
      "[Parallel(n_jobs=4)]: Done  16 tasks      | elapsed:  1.0min\n",
      "[CV] union__ingrs__ngram_range=(1, 4), union__ingrs__stop_words=None .\n",
      "[CV]  union__ingrs__ngram_range=(1, 3), union__ingrs__stop_words=english, score=0.7907081982049928, total=  17.8s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:  1.2min\n",
      "[CV] union__ingrs__ngram_range=(1, 4), union__ingrs__stop_words=None .\n",
      "[CV]  union__ingrs__ngram_range=(1, 3), union__ingrs__stop_words=english, score=0.797011771808029, total=  15.2s\n",
      "[Parallel(n_jobs=4)]: Done  18 out of  24 | elapsed:  1.3min remaining:   25.5s\n",
      "[CV] union__ingrs__ngram_range=(1, 4), union__ingrs__stop_words=english \n",
      "[CV]  union__ingrs__ngram_range=(1, 4), union__ingrs__stop_words=None, score=0.7955213752544673, total=  18.4s\n",
      "[Parallel(n_jobs=4)]: Done  19 out of  24 | elapsed:  1.4min remaining:   21.4s\n",
      "[CV] union__ingrs__ngram_range=(1, 4), union__ingrs__stop_words=english \n",
      "[CV]  union__ingrs__ngram_range=(1, 4), union__ingrs__stop_words=None, score=0.7928199713402218, total=  19.2s\n",
      "[CV] union__ingrs__ngram_range=(1, 4), union__ingrs__stop_words=english \n",
      "[Parallel(n_jobs=4)]: Done  20 out of  24 | elapsed:  1.4min remaining:   16.8s\n",
      "[CV]  union__ingrs__ngram_range=(1, 4), union__ingrs__stop_words=None, score=0.7976909145789315, total=  15.9s\n",
      "[Parallel(n_jobs=4)]: Done  21 out of  24 | elapsed:  1.6min remaining:   13.3s\n",
      "[CV]  union__ingrs__ngram_range=(1, 4), union__ingrs__stop_words=english, score=0.7954459775314785, total=  16.1s\n",
      "[Parallel(n_jobs=4)]: Done  22 out of  24 | elapsed:  1.6min remaining:    8.8s\n",
      "[CV]  union__ingrs__ngram_range=(1, 4), union__ingrs__stop_words=english, score=0.7928199713402218, total=  16.1s\n",
      "[CV]  union__ingrs__ngram_range=(1, 4), union__ingrs__stop_words=english, score=0.7976909145789315, total=  15.5s\n",
      "[Parallel(n_jobs=4)]: Done  24 out of  24 | elapsed:  1.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  24 out of  24 | elapsed:  1.7min finished\n",
      "180406-185828\n",
      "CPU times: user 23.8 s, sys: 482 ms, total: 24.3 s\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print_time()\n",
    "\n",
    "gsearch1.fit(train_ingrs, train_labels)\n",
    "\n",
    "print_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'union__ingrs__ngram_range': (1, 4), 'union__ingrs__stop_words': None}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79534408705787352"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting to test data (after encoding all of train+test ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.31 s, sys: 156 ms, total: 9.47 s\n",
      "Wall time: 9.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dvec_all = FeatureUnion([\n",
    "        (\"ingrs\", TfidfVectorizer(strip_accents='unicode',\n",
    "                                  tokenizer=itself,\n",
    "                                  preprocessor=itself)),\n",
    "        (\"words\", TfidfVectorizer(strip_accents='unicode',\n",
    "                                  preprocessor=combine_words,\n",
    "                                  ngram_range=(1, 4))),\n",
    "        ]).fit(get_ingrs(train+test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_bag = dvec_all.transform(get_ingrs(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<39774x907529 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3404836 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvec_all.transform(train_ingrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_linear =  LinearSVC(loss='hinge', C=10**0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26 s, sys: 135 ms, total: 26.2 s\n",
      "Wall time: 26.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svc_linear = svc_linear.fit(dvec_all.transform(train_ingrs), train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['brazilian', 'british', 'cajun_creole', 'chinese', 'filipino',\n",
       "       'french', 'greek', 'indian', 'irish', 'italian', 'jamaican',\n",
       "       'japanese', 'korean', 'mexican', 'moroccan', 'russian',\n",
       "       'southern_us', 'spanish', 'thai', 'vietnamese'],\n",
       "      dtype='<U12')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_linear.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_preds = svc_linear.predict(dvec_all.transform(get_ingrs(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9944,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ids = [r['id'] for r in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame([test_ids, list(test_preds)]).transpose()\n",
    "df_test.columns = ['id', 'cuisine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.to_csv('../_data/180406_ingr_14ngram_linearsvc_gridc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.79605\n",
    "### rank 270/1388\n",
    "![Kaggle](../_images/180406_14ngram_ingr_linearsvc.png)\n",
    "![Kaggle](../_images/180406_14ngram_ingr_linearsvc_standing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible improvements\n",
    "* Grid search on hyperparameters (kernel type, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
